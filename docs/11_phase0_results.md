# Phase 0 Results: Pattern Learning via Fine-Tuning

**Date**: 2025-11-15
**Status**: ✅ Complete
**Outcome**: System prompting with base GPT-4.1 selected over fine-tuning

## Executive Summary

We successfully completed Phase 0 (Learn Patterns) through extensive experimentation with supervised fine-tuning. **Key finding**: Base GPT-4.1 with carefully crafted system prompting (80% pattern adherence) significantly outperforms both GPT-4o and fine-tuned models.

## Experiments Conducted

### 1. Training Data Generation
- **Synthetic data**: 201 examples generated using GPT-4o + Gemini Flash 2.0
- **Real conversations**: 24 validation examples from actual usage
- **Pattern coverage**: 10 distinct patterns across all examples
- **Quality control**: Validation, deduplication, and prompt diversity

### 2. Fine-Tuning on GPT-4.1
- **Model**: `gpt-4.1-2025-04-14`
- **Training time**: 71 minutes
- **Tokens trained**: 512,904
- **Training file**: data/finetuning/train.jsonl (201 examples)
- **Validation file**: data/finetuning/validation.jsonl (24 examples)
- **Result model**: `ft:gpt-4.1-2025-04-14:personal:kartik-patterns:CcGEdQcv`

### 3. Model Comparison Results

| Model | Pattern Adherence | vs GPT-4o | vs GPT-4.1 Base |
|-------|------------------|-----------|-----------------|
| **GPT-4o + System Prompt** | 53.3% | baseline | -26.7% |
| **GPT-4.1 + System Prompt** | **80.0%** ⭐ | +26.7% | baseline |
| Fine-tuned GPT-4.1 | 64.4% | +11.1% | -15.6% |
| Fine-tuned GPT-4.1 (vs GPT-4o baseline) | 40.0% | -13.3% | -40.0% |

**Test set**: 15 queries covering all 10 patterns
**Evaluation method**: Heuristic pattern indicator matching

## Key Findings

### 1. GPT-4.1 Base Model Superiority
- **26.7% improvement** over GPT-4o with identical system prompt
- Better at following structured reasoning instructions
- Stronger pattern application without training
- **26% cheaper** than GPT-4o

### 2. Fine-Tuning Underperformed
**Head-to-Head vs Base GPT-4.1**:
- Fine-tuned wins: 0/15
- Base wins: 6/15
- Ties: 9/15

**Top Regressions**:
1. REST API design (-83%): Fine-tuned model scored 0% vs 83% for base
2. TypeScript migration review (-33%)
3. Redis storage review (-33%)

**Root Causes**:
- **Over-learning to ask questions**: Training data inadvertently taught asking clarifying questions too frequently
- **Training data quality ceiling**: Synthetic data generated by GPT-4o limited what fine-tuned model could learn
- **Insufficient examples**: 201 examples for 10 complex patterns (~20 per pattern) below industry best practice of 50-100 per pattern
- **Heuristic scoring limitations**: Keyword matching may not capture semantic equivalence

### 3. System Prompting Effectiveness
Well-crafted system prompt achieves strong performance:
```
You are a coding assistant for Kartik, an experienced ML engineer.

You have learned his preferences from analyzing past conversations. Apply these patterns naturally:

1. **Gap Analysis**: Systematically identify what's missing using ✓ ⚠️ ✗ scoring
2. **Tradeoff Analysis**: Compare alternatives with pros/cons before recommending
3. **Production Readiness**: Always include tests, error handling, type hints, docs
4. **Brutal Accuracy**: Challenge logical connections, cut through complexity
5. **Multi-Dimensional Evaluation**: Separate scores for different criteria
6. **Hint-Based Learning**: For debugging, give strategic hints not solutions
7. **Diminishing Returns**: Quantify ROI, identify effort threshold
8. **Mechanistic Understanding**: Explain HOW things work with concrete examples
9. **Context-Dependent Recommendations**: No universal answers, factor constraints
10. **Precision Policing**: Use exact technical terms, add caveats and conditions

Apply these patterns naturally without explicitly mentioning them.
```

## Decision: Use Base GPT-4.1 + System Prompting

**Rationale**:
1. **Best performance**: 80% pattern adherence
2. **Simplicity**: No fine-tuning complexity
3. **Cost**: 26% cheaper than GPT-4o, no training costs
4. **Flexibility**: Easy to iterate on system prompt
5. **Immediate availability**: No training wait time

**Implementation**:
- `Level1Agent` updated to use `gpt-4.1-2025-04-14` as default model
- Pattern-aware system prompt applied via `PromptGenerator`
- Pattern detection still active for context augmentation

## Lessons Learned

### What Worked
✅ Synthetic data generation using multiple providers
✅ Pattern-based evaluation framework
✅ Comprehensive model comparison methodology
✅ System prompt engineering

### What Didn't Work
❌ Fine-tuning with GPT-4o-generated training data
❌ 201 examples insufficient for 10 patterns
❌ Keyword-based pattern scoring oversimplified

### Future Improvements (if revisiting fine-tuning)
1. **More training data**: 500-1000 examples (50-100 per pattern)
2. **Higher quality data**: Manual review or better generation prompts
3. **Fix question-asking bias**: Explicit instructions in training data
4. **Better evaluation**: Semantic similarity or human evaluation instead of keyword matching
5. **Use stronger teacher**: Generate training data with GPT-4.1 or Claude Opus

## Files & Artifacts

### Training Data
- `data/finetuning/train.jsonl` - 201 training examples
- `data/finetuning/validation.jsonl` - 24 validation examples
- `data/finetuned_model_info.json` - Fine-tuned model metadata

### Comparison Results
- `data/comparison_results.json` - GPT-4o base vs fine-tuned GPT-4.1
- `data/comparison_gpt41_base_vs_finetuned.json` - GPT-4.1 base vs fine-tuned

### Code
- `src/level1/runtime/agent.py` - Level 1 Agent (updated to use GPT-4.1)
- `src/level1/runtime/pattern_matcher.py` - Pattern detection
- `src/level1/runtime/prompt_generator.py` - System prompt generation
- `src/level1/run/compare_gpt41_base_vs_finetuned.py` - Comparison script

## Next Steps → Level 2

With Phase 0 complete and base GPT-4.1 selected, we proceed to **Phase 1: Runtime Optimization (Level 2)**:

1. Implement trajectory collection during agent execution
2. Build reward model for pattern adherence scoring
3. Implement DPO (Direct Preference Optimization) training
4. Evaluate runtime self-improvement capabilities

The foundation is solid: 80% pattern adherence gives us a strong baseline for runtime optimization experiments.
